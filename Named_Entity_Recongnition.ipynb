{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Named Entity Recongnition.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEn5jvj1__vf",
        "colab_type": "text"
      },
      "source": [
        "Named Entity Recognition, also known as entity extraction classifies named entities that are present in a text into pre-defined categories like “individuals”, “companies”, “places”, “organization”, “cities”, “dates”, “product terminologies” etc. It adds a wealth of semantic knowledge to your content and helps you to promptly understand the subject of any given text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQEHk2q-AHuw",
        "colab_type": "text"
      },
      "source": [
        "Standard Libraries to use Named Entity Recognition\n",
        "\n",
        "I will discuss three standard libraries which are used a lot in Python to perform NER. I am sure there are many more and would encourage readers to add them in the comment section.\n",
        "\n",
        "    Standford NER\n",
        "    spaCy\n",
        "    NLTK"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAg_98NAB0yV",
        "colab_type": "text"
      },
      "source": [
        "**Standford NER**\n",
        "lets use standford NER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RnQB1p064CFm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "article = '''\n",
        "Asian shares skidded on Tuesday after a rout in tech stocks put Wall Street to the sword, while a \n",
        "sharp drop in oil prices and political risks in Europe pushed the dollar to 16-month highs as investors dumped \n",
        "riskier assets. MSCI’s broadest index of Asia-Pacific shares outside Japan dropped 1.7 percent to a 1-1/2 \n",
        "week trough, with Australian shares sinking 1.6 percent. Japan’s Nikkei dived 3.1 percent led by losses in \n",
        "electric machinery makers and suppliers of Apple’s iphone parts. Sterling fell to $1.286 after three straight \n",
        "sessions of losses took it to the lowest since Nov.1 as there were still considerable unresolved issues with the\n",
        "European Union over Brexit, British Prime Minister Theresa May said on Monday.'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0YQmeAB4ZK-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "outputId": "2313f8af-d664-433a-a1ed-92d4eae5887e"
      },
      "source": [
        "!wget https://nlp.stanford.edu/software/stanford-ner-2018-10-16.zip"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-06-19 18:37:53--  https://nlp.stanford.edu/software/stanford-ner-2018-10-16.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 180358328 (172M) [application/zip]\n",
            "Saving to: ‘stanford-ner-2018-10-16.zip’\n",
            "\n",
            "stanford-ner-2018-1 100%[===================>] 172.00M  1.47MB/s    in 2m 18s  \n",
            "\n",
            "2019-06-19 18:40:11 (1.25 MB/s) - ‘stanford-ner-2018-10-16.zip’ saved [180358328/180358328]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OEYwd1v4vns",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip stanford-ner-2018-10-16.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zn9waGlr4Hxa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "from nltk.tag import StanfordNERTagger\n",
        "\n",
        "print('NTLK Version: %s' % nltk.__version__)\n",
        "\n",
        "stanford_ner_tagger = StanfordNERTagger(\n",
        "    'stanford-ner-2018-10-16/' + 'classifiers/english.muc.7class.distsim.crf.ser.gz',\n",
        "    'stanford-ner-2018-10-16/' + 'stanford-ner-3.9.2.jar'\n",
        ")\n",
        "\n",
        "results = stanford_ner_tagger.tag(article.split())\n",
        "\n",
        "print('Original Sentence: %s' % (article))\n",
        "for result in results:\n",
        "    tag_value = result[0]\n",
        "    tag_type = result[1]\n",
        "    if tag_type != 'O':\n",
        "      print('Type: %s, Value: %s' %(tag_type, tag_value))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APbZSNi8DHoT",
        "colab_type": "text"
      },
      "source": [
        "**spaCy**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nM4c0u6u4J_3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download en"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yau_YTgj6w0E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "spacy_nlp=spacy.load(\"en\")\n",
        "result=spacy_nlp(article)\n",
        "\n",
        "for element in result.ents:\n",
        "  print('Type: %s, Value: %s' % (element.label_, element))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kU-uFWAO83Lb",
        "colab_type": "text"
      },
      "source": [
        "NLTK (Natural Language Toolkit) is a Python package that provides a set of natural languages corpora and APIs of wide varieties of NLP algorithms.\n",
        "\n",
        "To perform Named Entity Recognition using NLTK, it needs to be done in three stages —\n",
        "\n",
        "    Work Tokenization\n",
        "    Parts of Speech (POS) tagging\n",
        "    Named Entity Recognition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rN3W1w_m9272",
        "colab_type": "text"
      },
      "source": [
        "Note, we need to download some standard corpora and API from NLTK to perform parts of speech tagging and named entity recognition. Hence, we downloaded these from nltk in the above Python code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7uTsoTeW899e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "from nltk import word_tokenize, pos_tag, ne_chunk\n",
        "nltk.download(\"words\")\n",
        "nltk.download(\"averaged_perceptron_tagger\")\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"maxent_ne_chunker\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e91Z0wmZ7Gpi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess(text):\n",
        "  text=nltk.word_tokenize(text)\n",
        "  result=nltk.pos_tag(text)\n",
        "  return result\n",
        "\n",
        "\n",
        "result=preprocess(article)\n",
        "result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbV-S8F6-ic3",
        "colab_type": "text"
      },
      "source": [
        "Now once we have done the parts-of-speech tagging we will be doing a process called chunking. Text chunking is also called as shallow parsing which typically follows POS tagging to add more structure to the sentence. The result is grouping of words in “chunks”.\n",
        "\n",
        "So, lets perform chunking to our article which we have already POS tagged.\n",
        "\n",
        "Our target here would be to NER tag only the Nouns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-adFiEz-P1Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for x in str(result).split('\\n'):\n",
        "  if '/NN' in x:\n",
        "    print(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1ElrF9a_Orw",
        "colab_type": "text"
      },
      "source": [
        "The output looks decent but not great. Say we take up a little more complex task.\n",
        "\n",
        "    Say, we want to implement noun phrase chunking to identify named entities.\n",
        "\n",
        "    Our chunk pattern consists of one rule, that a noun phrase, NP, should be formed whenever the chunker finds an optional determiner, DT, followed by any number of adjectives, JJ, and then a noun, NN."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hiIGNQFd_DB7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pattern='NP: {<DT>?<JJ>*<NN>}'\n",
        "cp=nltk.RegexpParser(pattern)\n",
        "cs=cp.parse(result)\n",
        "print(cs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROog9YHa_nqU",
        "colab_type": "text"
      },
      "source": [
        "The output can be read as a tree with “S” means the sentence as the first level. It can viewed in a more acceptable format called IOB tags (Inside, Outside, Beginning)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZJPnNm8_eMb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.chunk import conlltags2tree, tree2conlltags\n",
        "from pprint import pprint\n",
        "\n",
        "iob_tagged = tree2conlltags(cs)\n",
        "\n",
        "pprint(iob_tagged)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXU1QL2I_2J_",
        "colab_type": "text"
      },
      "source": [
        "Here, in the output each token is a line with parts-of-speech and named entity tagged. If you want to extract the IOB tags, as it is a tuple you simply do-"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ha5Jom4H_wdt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for word, pos, ner in iob_tagged:\n",
        "    print(word, pos, ner)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}