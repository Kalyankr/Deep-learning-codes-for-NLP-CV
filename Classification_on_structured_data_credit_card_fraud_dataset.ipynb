{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Classification on structured data: credit card fraud dataset.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTPmTZKc6770",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tf-nightly-gpu-2.0-preview"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRHmSyHxEIhN",
        "colab_type": "text"
      },
      "source": [
        "## First, vectorize the CSV data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NG5iYmD-7dZh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "outputId": "e7c3087b-25e1-4b67-e118-5ffd9c51a068"
      },
      "source": [
        "import csv\n",
        "import numpy as np\n",
        "\n",
        "# Get the real data from https://www.kaggle.com/mlg-ulb/creditcardfraud/downloads/creditcardfraud.zip/\n",
        "fname = 'creditcard.csv'\n",
        "\n",
        "all_features = []\n",
        "all_targets = []\n",
        "with open(fname) as f:\n",
        "  for i, line in enumerate(f):\n",
        "    if i == 0:\n",
        "      print('HEADER:', line.strip())\n",
        "      continue  # Skip header\n",
        "    fields = line.strip().split(',')\n",
        "    all_features.append([float(v.replace('\"', '')) for v in fields[:-1]])\n",
        "    all_targets.append([int(fields[-1].replace('\"', ''))])\n",
        "    if i == 1:\n",
        "      print('EXAMPLE FEATURES:', all_features[-1])\n",
        "    \n",
        "features = np.array(all_features, dtype='float32')\n",
        "targets = np.array(all_targets, dtype='uint8')\n",
        "print('features.shape:', features.shape)\n",
        "print('targets.shape:', targets.shape)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "HEADER: \"Time\",\"V1\",\"V2\",\"V3\",\"V4\",\"V5\",\"V6\",\"V7\",\"V8\",\"V9\",\"V10\",\"V11\",\"V12\",\"V13\",\"V14\",\"V15\",\"V16\",\"V17\",\"V18\",\"V19\",\"V20\",\"V21\",\"V22\",\"V23\",\"V24\",\"V25\",\"V26\",\"V27\",\"V28\",\"Amount\",\"Class\"\n",
            "EXAMPLE FEATURES: [0.0, -1.3598071336738, -0.0727811733098497, 2.53634673796914, 1.37815522427443, -0.338320769942518, 0.462387777762292, 0.239598554061257, 0.0986979012610507, 0.363786969611213, 0.0907941719789316, -0.551599533260813, -0.617800855762348, -0.991389847235408, -0.311169353699879, 1.46817697209427, -0.470400525259478, 0.207971241929242, 0.0257905801985591, 0.403992960255733, 0.251412098239705, -0.018306777944153, 0.277837575558899, -0.110473910188767, 0.0669280749146731, 0.128539358273528, -0.189114843888824, 0.133558376740387, -0.0210530534538215, 149.62]\n",
            "features.shape: (284807, 30)\n",
            "targets.shape: (284807, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymKKJ7MLEMJZ",
        "colab_type": "text"
      },
      "source": [
        "## Prepare a validation set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3FDeMVhH7tLt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_val_samples = int(len(features) * 0.2)\n",
        "train_features = features[:-num_val_samples]\n",
        "train_targets = targets[:-num_val_samples]\n",
        "val_features = features[-num_val_samples:]\n",
        "val_targets = targets[-num_val_samples:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYMuiKYzInMy",
        "colab_type": "code",
        "outputId": "064abcbd-9a43-487c-ab12-40141f0552c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print('Number of training samples:', len(train_features))\n",
        "print('Number of validation samples:', len(val_features))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training samples: 227846\n",
            "Number of validation samples: 56961\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWKB_CVZFLpB",
        "colab_type": "text"
      },
      "source": [
        "## Analyze class imbalance in the targets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKWfTCcyFN2K",
        "colab_type": "code",
        "outputId": "89d5951c-e6a0-46a9-e692-978369ec4445",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "counts = np.bincount(train_targets[:, 0])\n",
        "print('Number of positive samples in training data: {} ({:.2f}% of total)'.format(counts[1], 100 * float(counts[1]) / len(train_targets)))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of positive samples in training data: 417 (0.18% of total)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qjGWErngGny7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "weight_for_0 = 1. / counts[0]\n",
        "weight_for_1 = 1. / counts[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNa3j61UEOd7",
        "colab_type": "text"
      },
      "source": [
        "## Normalize the data using training set statistics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hkdwBiriD4Oc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mean = np.mean(train_features, axis=0)\n",
        "train_features -= mean\n",
        "val_features -= mean\n",
        "std = np.std(train_features, axis=0)\n",
        "train_features /= std\n",
        "val_features /= std"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hIMJ6ebpD6qG",
        "colab_type": "code",
        "outputId": "059204f6-140f-4a02-c4b7-4d14541f0034",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "from tensorflow import keras\n",
        "\n",
        "model = keras.Sequential([\n",
        "  keras.layers.Dense(256, activation='relu',\n",
        "                     input_shape=(train_features.shape[-1],)),\n",
        "  keras.layers.Dense(256, activation='relu'),\n",
        "  keras.layers.Dropout(0.3),\n",
        "  keras.layers.Dense(256, activation='relu'),\n",
        "  keras.layers.Dropout(0.3),\n",
        "  keras.layers.Dense(1, activation='sigmoid'),\n",
        "])\n",
        "model.summary()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_4 (Dense)              (None, 256)               7936      \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 1)                 257       \n",
            "=================================================================\n",
            "Total params: 139,777\n",
            "Trainable params: 139,777\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "REP1KlrSEx-m",
        "colab_type": "code",
        "outputId": "36de3c8a-abd9-457e-90e9-6511ef18a930",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1943
        }
      },
      "source": [
        "metrics = [keras.metrics.FalseNegatives(name='fn'),\n",
        "           keras.metrics.FalsePositives(name='fp'),\n",
        "           keras.metrics.TrueNegatives(name='tn'),\n",
        "           keras.metrics.TruePositives(name='tp'),\n",
        "           keras.metrics.Precision(name='precision'),\n",
        "           keras.metrics.Recall(name='recall')]\n",
        "\n",
        "model.compile(optimizer=keras.optimizers.Adam(1e-2),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=metrics)\n",
        "\n",
        "callbacks = [keras.callbacks.ModelCheckpoint('fraud_model_at_epoch_{epoch}.h5')]\n",
        "class_weight = {0: weight_for_0, 1: weight_for_1}\n",
        "\n",
        "model.fit(train_features, train_targets,\n",
        "          batch_size=2048,\n",
        "          epochs=50,\n",
        "          verbose=2,\n",
        "          callbacks=callbacks,\n",
        "          validation_data=(val_features, val_targets),\n",
        "          class_weight=class_weight)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 227846 samples, validate on 56961 samples\n",
            "Epoch 1/50\n",
            "227846/227846 - 1s - loss: 2.2051e-06 - fn: 43.0000 - fp: 25974.0000 - tn: 201455.0000 - tp: 374.0000 - precision: 0.0142 - recall: 0.8969 - val_loss: 0.1223 - val_fn: 9.0000 - val_fp: 1034.0000 - val_tn: 55852.0000 - val_tp: 66.0000 - val_precision: 0.0600 - val_recall: 0.8800\n",
            "Epoch 2/50\n",
            "227846/227846 - 1s - loss: 1.5369e-06 - fn: 43.0000 - fp: 8030.0000 - tn: 219399.0000 - tp: 374.0000 - precision: 0.0445 - recall: 0.8969 - val_loss: 0.1107 - val_fn: 8.0000 - val_fp: 996.0000 - val_tn: 55890.0000 - val_tp: 67.0000 - val_precision: 0.0630 - val_recall: 0.8933\n",
            "Epoch 3/50\n",
            "227846/227846 - 1s - loss: 1.2256e-06 - fn: 28.0000 - fp: 7297.0000 - tn: 220132.0000 - tp: 389.0000 - precision: 0.0506 - recall: 0.9329 - val_loss: 0.0599 - val_fn: 11.0000 - val_fp: 718.0000 - val_tn: 56168.0000 - val_tp: 64.0000 - val_precision: 0.0818 - val_recall: 0.8533\n",
            "Epoch 4/50\n",
            "227846/227846 - 1s - loss: 1.0485e-06 - fn: 28.0000 - fp: 6077.0000 - tn: 221352.0000 - tp: 389.0000 - precision: 0.0602 - recall: 0.9329 - val_loss: 0.0512 - val_fn: 8.0000 - val_fp: 840.0000 - val_tn: 56046.0000 - val_tp: 67.0000 - val_precision: 0.0739 - val_recall: 0.8933\n",
            "Epoch 5/50\n",
            "227846/227846 - 1s - loss: 8.9432e-07 - fn: 20.0000 - fp: 7260.0000 - tn: 220169.0000 - tp: 397.0000 - precision: 0.0518 - recall: 0.9520 - val_loss: 0.1861 - val_fn: 6.0000 - val_fp: 3166.0000 - val_tn: 53720.0000 - val_tp: 69.0000 - val_precision: 0.0213 - val_recall: 0.9200\n",
            "Epoch 6/50\n",
            "227846/227846 - 1s - loss: 8.6660e-07 - fn: 23.0000 - fp: 7422.0000 - tn: 220007.0000 - tp: 394.0000 - precision: 0.0504 - recall: 0.9448 - val_loss: 0.0476 - val_fn: 8.0000 - val_fp: 744.0000 - val_tn: 56142.0000 - val_tp: 67.0000 - val_precision: 0.0826 - val_recall: 0.8933\n",
            "Epoch 7/50\n",
            "227846/227846 - 1s - loss: 7.2182e-07 - fn: 16.0000 - fp: 6208.0000 - tn: 221221.0000 - tp: 401.0000 - precision: 0.0607 - recall: 0.9616 - val_loss: 0.0379 - val_fn: 7.0000 - val_fp: 717.0000 - val_tn: 56169.0000 - val_tp: 68.0000 - val_precision: 0.0866 - val_recall: 0.9067\n",
            "Epoch 8/50\n",
            "227846/227846 - 1s - loss: 7.1953e-07 - fn: 14.0000 - fp: 7456.0000 - tn: 219973.0000 - tp: 403.0000 - precision: 0.0513 - recall: 0.9664 - val_loss: 0.1245 - val_fn: 6.0000 - val_fp: 2912.0000 - val_tn: 53974.0000 - val_tp: 69.0000 - val_precision: 0.0231 - val_recall: 0.9200\n",
            "Epoch 9/50\n",
            "227846/227846 - 1s - loss: 6.6771e-07 - fn: 14.0000 - fp: 7569.0000 - tn: 219860.0000 - tp: 403.0000 - precision: 0.0506 - recall: 0.9664 - val_loss: 0.0388 - val_fn: 10.0000 - val_fp: 697.0000 - val_tn: 56189.0000 - val_tp: 65.0000 - val_precision: 0.0853 - val_recall: 0.8667\n",
            "Epoch 10/50\n",
            "227846/227846 - 1s - loss: 6.9433e-07 - fn: 13.0000 - fp: 6277.0000 - tn: 221152.0000 - tp: 404.0000 - precision: 0.0605 - recall: 0.9688 - val_loss: 0.0433 - val_fn: 8.0000 - val_fp: 662.0000 - val_tn: 56224.0000 - val_tp: 67.0000 - val_precision: 0.0919 - val_recall: 0.8933\n",
            "Epoch 11/50\n",
            "227846/227846 - 1s - loss: 6.5010e-07 - fn: 11.0000 - fp: 6150.0000 - tn: 221279.0000 - tp: 406.0000 - precision: 0.0619 - recall: 0.9736 - val_loss: 0.0387 - val_fn: 10.0000 - val_fp: 459.0000 - val_tn: 56427.0000 - val_tp: 65.0000 - val_precision: 0.1240 - val_recall: 0.8667\n",
            "Epoch 12/50\n",
            "227846/227846 - 1s - loss: 4.6471e-07 - fn: 9.0000 - fp: 4499.0000 - tn: 222930.0000 - tp: 408.0000 - precision: 0.0831 - recall: 0.9784 - val_loss: 0.0411 - val_fn: 12.0000 - val_fp: 706.0000 - val_tn: 56180.0000 - val_tp: 63.0000 - val_precision: 0.0819 - val_recall: 0.8400\n",
            "Epoch 13/50\n",
            "227846/227846 - 1s - loss: 4.6764e-07 - fn: 7.0000 - fp: 4829.0000 - tn: 222600.0000 - tp: 410.0000 - precision: 0.0783 - recall: 0.9832 - val_loss: 0.0950 - val_fn: 6.0000 - val_fp: 2619.0000 - val_tn: 54267.0000 - val_tp: 69.0000 - val_precision: 0.0257 - val_recall: 0.9200\n",
            "Epoch 14/50\n",
            "227846/227846 - 1s - loss: 5.0388e-07 - fn: 10.0000 - fp: 5212.0000 - tn: 222217.0000 - tp: 407.0000 - precision: 0.0724 - recall: 0.9760 - val_loss: 0.0405 - val_fn: 10.0000 - val_fp: 716.0000 - val_tn: 56170.0000 - val_tp: 65.0000 - val_precision: 0.0832 - val_recall: 0.8667\n",
            "Epoch 15/50\n",
            "227846/227846 - 1s - loss: 4.0100e-07 - fn: 7.0000 - fp: 4516.0000 - tn: 222913.0000 - tp: 410.0000 - precision: 0.0832 - recall: 0.9832 - val_loss: 0.0181 - val_fn: 11.0000 - val_fp: 276.0000 - val_tn: 56610.0000 - val_tp: 64.0000 - val_precision: 0.1882 - val_recall: 0.8533\n",
            "Epoch 16/50\n",
            "227846/227846 - 1s - loss: 4.0482e-07 - fn: 8.0000 - fp: 3801.0000 - tn: 223628.0000 - tp: 409.0000 - precision: 0.0971 - recall: 0.9808 - val_loss: 0.0227 - val_fn: 11.0000 - val_fp: 424.0000 - val_tn: 56462.0000 - val_tp: 64.0000 - val_precision: 0.1311 - val_recall: 0.8533\n",
            "Epoch 17/50\n",
            "227846/227846 - 1s - loss: 5.2965e-07 - fn: 8.0000 - fp: 5681.0000 - tn: 221748.0000 - tp: 409.0000 - precision: 0.0672 - recall: 0.9808 - val_loss: 0.0341 - val_fn: 10.0000 - val_fp: 702.0000 - val_tn: 56184.0000 - val_tp: 65.0000 - val_precision: 0.0847 - val_recall: 0.8667\n",
            "Epoch 18/50\n",
            "227846/227846 - 1s - loss: 6.1515e-07 - fn: 7.0000 - fp: 5922.0000 - tn: 221507.0000 - tp: 410.0000 - precision: 0.0648 - recall: 0.9832 - val_loss: 0.0369 - val_fn: 8.0000 - val_fp: 743.0000 - val_tn: 56143.0000 - val_tp: 67.0000 - val_precision: 0.0827 - val_recall: 0.8933\n",
            "Epoch 19/50\n",
            "227846/227846 - 1s - loss: 3.0188e-06 - fn: 21.0000 - fp: 12276.0000 - tn: 215153.0000 - tp: 396.0000 - precision: 0.0312 - recall: 0.9496 - val_loss: 0.2960 - val_fn: 7.0000 - val_fp: 2193.0000 - val_tn: 54693.0000 - val_tp: 68.0000 - val_precision: 0.0301 - val_recall: 0.9067\n",
            "Epoch 20/50\n",
            "227846/227846 - 1s - loss: 1.3643e-06 - fn: 19.0000 - fp: 8824.0000 - tn: 218605.0000 - tp: 398.0000 - precision: 0.0432 - recall: 0.9544 - val_loss: 0.1125 - val_fn: 10.0000 - val_fp: 1356.0000 - val_tn: 55530.0000 - val_tp: 65.0000 - val_precision: 0.0457 - val_recall: 0.8667\n",
            "Epoch 21/50\n",
            "227846/227846 - 1s - loss: 9.7217e-07 - fn: 18.0000 - fp: 7582.0000 - tn: 219847.0000 - tp: 399.0000 - precision: 0.0500 - recall: 0.9568 - val_loss: 0.0928 - val_fn: 7.0000 - val_fp: 1714.0000 - val_tn: 55172.0000 - val_tp: 68.0000 - val_precision: 0.0382 - val_recall: 0.9067\n",
            "Epoch 22/50\n",
            "227846/227846 - 1s - loss: 4.9744e-07 - fn: 8.0000 - fp: 5507.0000 - tn: 221922.0000 - tp: 409.0000 - precision: 0.0691 - recall: 0.9808 - val_loss: 0.0218 - val_fn: 11.0000 - val_fp: 364.0000 - val_tn: 56522.0000 - val_tp: 64.0000 - val_precision: 0.1495 - val_recall: 0.8533\n",
            "Epoch 23/50\n",
            "227846/227846 - 1s - loss: 7.2604e-07 - fn: 6.0000 - fp: 4875.0000 - tn: 222554.0000 - tp: 411.0000 - precision: 0.0778 - recall: 0.9856 - val_loss: 0.0958 - val_fn: 9.0000 - val_fp: 912.0000 - val_tn: 55974.0000 - val_tp: 66.0000 - val_precision: 0.0675 - val_recall: 0.8800\n",
            "Epoch 24/50\n",
            "227846/227846 - 1s - loss: 6.2485e-07 - fn: 11.0000 - fp: 5412.0000 - tn: 222017.0000 - tp: 406.0000 - precision: 0.0698 - recall: 0.9736 - val_loss: 0.0504 - val_fn: 9.0000 - val_fp: 701.0000 - val_tn: 56185.0000 - val_tp: 66.0000 - val_precision: 0.0860 - val_recall: 0.8800\n",
            "Epoch 25/50\n",
            "227846/227846 - 1s - loss: 4.4275e-07 - fn: 4.0000 - fp: 4390.0000 - tn: 223039.0000 - tp: 413.0000 - precision: 0.0860 - recall: 0.9904 - val_loss: 0.0270 - val_fn: 11.0000 - val_fp: 266.0000 - val_tn: 56620.0000 - val_tp: 64.0000 - val_precision: 0.1939 - val_recall: 0.8533\n",
            "Epoch 26/50\n",
            "227846/227846 - 1s - loss: 3.5077e-07 - fn: 3.0000 - fp: 3114.0000 - tn: 224315.0000 - tp: 414.0000 - precision: 0.1173 - recall: 0.9928 - val_loss: 0.0202 - val_fn: 12.0000 - val_fp: 215.0000 - val_tn: 56671.0000 - val_tp: 63.0000 - val_precision: 0.2266 - val_recall: 0.8400\n",
            "Epoch 27/50\n",
            "227846/227846 - 1s - loss: 2.6295e-07 - fn: 2.0000 - fp: 2578.0000 - tn: 224851.0000 - tp: 415.0000 - precision: 0.1387 - recall: 0.9952 - val_loss: 0.0471 - val_fn: 9.0000 - val_fp: 839.0000 - val_tn: 56047.0000 - val_tp: 66.0000 - val_precision: 0.0729 - val_recall: 0.8800\n",
            "Epoch 28/50\n",
            "227846/227846 - 1s - loss: 2.7546e-07 - fn: 2.0000 - fp: 3221.0000 - tn: 224208.0000 - tp: 415.0000 - precision: 0.1141 - recall: 0.9952 - val_loss: 0.0257 - val_fn: 9.0000 - val_fp: 386.0000 - val_tn: 56500.0000 - val_tp: 66.0000 - val_precision: 0.1460 - val_recall: 0.8800\n",
            "Epoch 29/50\n",
            "227846/227846 - 1s - loss: 2.2066e-07 - fn: 2.0000 - fp: 2160.0000 - tn: 225269.0000 - tp: 415.0000 - precision: 0.1612 - recall: 0.9952 - val_loss: 0.0204 - val_fn: 9.0000 - val_fp: 223.0000 - val_tn: 56663.0000 - val_tp: 66.0000 - val_precision: 0.2284 - val_recall: 0.8800\n",
            "Epoch 30/50\n",
            "227846/227846 - 1s - loss: 1.6686e-07 - fn: 1.0000 - fp: 1578.0000 - tn: 225851.0000 - tp: 416.0000 - precision: 0.2086 - recall: 0.9976 - val_loss: 0.0557 - val_fn: 9.0000 - val_fp: 791.0000 - val_tn: 56095.0000 - val_tp: 66.0000 - val_precision: 0.0770 - val_recall: 0.8800\n",
            "Epoch 31/50\n",
            "227846/227846 - 1s - loss: 2.8947e-07 - fn: 4.0000 - fp: 2125.0000 - tn: 225304.0000 - tp: 413.0000 - precision: 0.1627 - recall: 0.9904 - val_loss: 0.0563 - val_fn: 8.0000 - val_fp: 707.0000 - val_tn: 56179.0000 - val_tp: 67.0000 - val_precision: 0.0866 - val_recall: 0.8933\n",
            "Epoch 32/50\n",
            "227846/227846 - 1s - loss: 4.6632e-06 - fn: 28.0000 - fp: 11541.0000 - tn: 215888.0000 - tp: 389.0000 - precision: 0.0326 - recall: 0.9329 - val_loss: 0.3997 - val_fn: 9.0000 - val_fp: 1759.0000 - val_tn: 55127.0000 - val_tp: 66.0000 - val_precision: 0.0362 - val_recall: 0.8800\n",
            "Epoch 33/50\n",
            "227846/227846 - 1s - loss: 2.7113e-06 - fn: 13.0000 - fp: 11382.0000 - tn: 216047.0000 - tp: 404.0000 - precision: 0.0343 - recall: 0.9688 - val_loss: 0.5349 - val_fn: 12.0000 - val_fp: 2469.0000 - val_tn: 54417.0000 - val_tp: 63.0000 - val_precision: 0.0249 - val_recall: 0.8400\n",
            "Epoch 34/50\n",
            "227846/227846 - 1s - loss: 2.3526e-06 - fn: 9.0000 - fp: 10639.0000 - tn: 216790.0000 - tp: 408.0000 - precision: 0.0369 - recall: 0.9784 - val_loss: 0.3347 - val_fn: 9.0000 - val_fp: 1522.0000 - val_tn: 55364.0000 - val_tp: 66.0000 - val_precision: 0.0416 - val_recall: 0.8800\n",
            "Epoch 35/50\n",
            "227846/227846 - 1s - loss: 1.5474e-06 - fn: 10.0000 - fp: 8180.0000 - tn: 219249.0000 - tp: 407.0000 - precision: 0.0474 - recall: 0.9760 - val_loss: 0.1812 - val_fn: 10.0000 - val_fp: 1043.0000 - val_tn: 55843.0000 - val_tp: 65.0000 - val_precision: 0.0587 - val_recall: 0.8667\n",
            "Epoch 36/50\n",
            "227846/227846 - 1s - loss: 8.8775e-07 - fn: 10.0000 - fp: 6131.0000 - tn: 221298.0000 - tp: 407.0000 - precision: 0.0623 - recall: 0.9760 - val_loss: 0.1323 - val_fn: 8.0000 - val_fp: 887.0000 - val_tn: 55999.0000 - val_tp: 67.0000 - val_precision: 0.0702 - val_recall: 0.8933\n",
            "Epoch 37/50\n",
            "227846/227846 - 1s - loss: 5.7834e-07 - fn: 6.0000 - fp: 4089.0000 - tn: 223340.0000 - tp: 411.0000 - precision: 0.0913 - recall: 0.9856 - val_loss: 0.0264 - val_fn: 11.0000 - val_fp: 242.0000 - val_tn: 56644.0000 - val_tp: 64.0000 - val_precision: 0.2092 - val_recall: 0.8533\n",
            "Epoch 38/50\n",
            "227846/227846 - 1s - loss: 7.7592e-07 - fn: 7.0000 - fp: 4084.0000 - tn: 223345.0000 - tp: 410.0000 - precision: 0.0912 - recall: 0.9832 - val_loss: 0.0264 - val_fn: 10.0000 - val_fp: 241.0000 - val_tn: 56645.0000 - val_tp: 65.0000 - val_precision: 0.2124 - val_recall: 0.8667\n",
            "Epoch 39/50\n",
            "227846/227846 - 1s - loss: 7.3131e-07 - fn: 6.0000 - fp: 5628.0000 - tn: 221801.0000 - tp: 411.0000 - precision: 0.0681 - recall: 0.9856 - val_loss: 0.0520 - val_fn: 8.0000 - val_fp: 685.0000 - val_tn: 56201.0000 - val_tp: 67.0000 - val_precision: 0.0891 - val_recall: 0.8933\n",
            "Epoch 40/50\n",
            "227846/227846 - 1s - loss: 7.8293e-07 - fn: 8.0000 - fp: 4874.0000 - tn: 222555.0000 - tp: 409.0000 - precision: 0.0774 - recall: 0.9808 - val_loss: 0.0410 - val_fn: 9.0000 - val_fp: 408.0000 - val_tn: 56478.0000 - val_tp: 66.0000 - val_precision: 0.1392 - val_recall: 0.8800\n",
            "Epoch 41/50\n",
            "227846/227846 - 1s - loss: 3.5224e-07 - fn: 4.0000 - fp: 3244.0000 - tn: 224185.0000 - tp: 413.0000 - precision: 0.1129 - recall: 0.9904 - val_loss: 0.0248 - val_fn: 11.0000 - val_fp: 364.0000 - val_tn: 56522.0000 - val_tp: 64.0000 - val_precision: 0.1495 - val_recall: 0.8533\n",
            "Epoch 42/50\n",
            "227846/227846 - 1s - loss: 2.0582e-07 - fn: 1.0000 - fp: 2173.0000 - tn: 225256.0000 - tp: 416.0000 - precision: 0.1607 - recall: 0.9976 - val_loss: 0.0202 - val_fn: 11.0000 - val_fp: 307.0000 - val_tn: 56579.0000 - val_tp: 64.0000 - val_precision: 0.1725 - val_recall: 0.8533\n",
            "Epoch 43/50\n",
            "227846/227846 - 1s - loss: 3.7251e-07 - fn: 4.0000 - fp: 2192.0000 - tn: 225237.0000 - tp: 413.0000 - precision: 0.1585 - recall: 0.9904 - val_loss: 0.0163 - val_fn: 13.0000 - val_fp: 213.0000 - val_tn: 56673.0000 - val_tp: 62.0000 - val_precision: 0.2255 - val_recall: 0.8267\n",
            "Epoch 44/50\n",
            "227846/227846 - 1s - loss: 3.9369e-07 - fn: 4.0000 - fp: 2515.0000 - tn: 224914.0000 - tp: 413.0000 - precision: 0.1411 - recall: 0.9904 - val_loss: 0.1086 - val_fn: 8.0000 - val_fp: 944.0000 - val_tn: 55942.0000 - val_tp: 67.0000 - val_precision: 0.0663 - val_recall: 0.8933\n",
            "Epoch 45/50\n",
            "227846/227846 - 1s - loss: 6.2534e-07 - fn: 2.0000 - fp: 3365.0000 - tn: 224064.0000 - tp: 415.0000 - precision: 0.1098 - recall: 0.9952 - val_loss: 0.0485 - val_fn: 14.0000 - val_fp: 352.0000 - val_tn: 56534.0000 - val_tp: 61.0000 - val_precision: 0.1477 - val_recall: 0.8133\n",
            "Epoch 46/50\n",
            "227846/227846 - 1s - loss: 4.2772e-07 - fn: 4.0000 - fp: 2420.0000 - tn: 225009.0000 - tp: 413.0000 - precision: 0.1458 - recall: 0.9904 - val_loss: 0.0272 - val_fn: 10.0000 - val_fp: 341.0000 - val_tn: 56545.0000 - val_tp: 65.0000 - val_precision: 0.1601 - val_recall: 0.8667\n",
            "Epoch 47/50\n",
            "227846/227846 - 1s - loss: 3.0615e-07 - fn: 4.0000 - fp: 2497.0000 - tn: 224932.0000 - tp: 413.0000 - precision: 0.1419 - recall: 0.9904 - val_loss: 0.0275 - val_fn: 11.0000 - val_fp: 366.0000 - val_tn: 56520.0000 - val_tp: 64.0000 - val_precision: 0.1488 - val_recall: 0.8533\n",
            "Epoch 48/50\n",
            "227846/227846 - 1s - loss: 2.6875e-07 - fn: 2.0000 - fp: 1888.0000 - tn: 225541.0000 - tp: 415.0000 - precision: 0.1802 - recall: 0.9952 - val_loss: 0.0178 - val_fn: 13.0000 - val_fp: 144.0000 - val_tn: 56742.0000 - val_tp: 62.0000 - val_precision: 0.3010 - val_recall: 0.8267\n",
            "Epoch 49/50\n",
            "227846/227846 - 1s - loss: 1.4462e-07 - fn: 0.0000e+00 - fp: 1202.0000 - tn: 226227.0000 - tp: 417.0000 - precision: 0.2576 - recall: 1.0000 - val_loss: 0.0151 - val_fn: 11.0000 - val_fp: 136.0000 - val_tn: 56750.0000 - val_tp: 64.0000 - val_precision: 0.3200 - val_recall: 0.8533\n",
            "Epoch 50/50\n",
            "227846/227846 - 1s - loss: 3.5556e-07 - fn: 2.0000 - fp: 2618.0000 - tn: 224811.0000 - tp: 415.0000 - precision: 0.1368 - recall: 0.9952 - val_loss: 0.0285 - val_fn: 12.0000 - val_fp: 236.0000 - val_tn: 56650.0000 - val_tp: 63.0000 - val_precision: 0.2107 - val_recall: 0.8400\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fa95e82e320>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdzsLQQgJMib",
        "colab_type": "text"
      },
      "source": [
        "At the end of training, out of 56,961 validation transactions, we are:\n",
        "\n",
        "- Correctly identifying 66 of them as fraudulent\n",
        "- Missing 9 fraudulent transactions\n",
        "- At the cost of incorrectly flagging 441 legitimate transactions\n",
        "\n",
        "In the real world, one would put an even higher weight on class 1,\n",
        "so as to reflect that False Negatives are more costly than False Positives.\n",
        "\n",
        "Next time your credit card gets  declined in an online purchase -- this is why."
      ]
    }
  ]
}